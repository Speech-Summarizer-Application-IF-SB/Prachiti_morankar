{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM2fzZLQhz5hC+lannx9VDU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"DO4K6sVLsx9R","executionInfo":{"status":"ok","timestamp":1760277524126,"user_tz":-330,"elapsed":5105,"user":{"displayName":"Prachiti Morankar","userId":"16577217993084456597"}}},"outputs":[],"source":["!pip install --quiet torchaudio pydub numpy pandas scikit-learn tensorflow nltk"]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import os\n","\n","# Define the correct path to the training CSV file inside the unzipped structure\n","CORRECT_FILE_PATH = '/content/summarization_data/CSV/train.csv'\n","\n","# Check if the file exists before attempting to load\n","if os.path.exists(CORRECT_FILE_PATH):\n","    print(f\"✅ SUCCESS: Found training data at {CORRECT_FILE_PATH}. Loading...\")\n","\n","    # Load Summarization Data (Using the training subset)\n","    dialogsum_df = pd.read_csv(CORRECT_FILE_PATH)\n","\n","    # Add validation/test data if desired, but for simplified POC, we use only train data\n","    # Filter columns to only keep id, dialogue, and summary, as originally intended\n","    dialogsum_df = dialogsum_df[['dialogue', 'summary']].dropna()\n","\n","    # Sample a small subset for feasible 'from-scratch' training\n","    # The original plan sampled 2000 rows.\n","    SAMPLE_SIZE = 2000\n","    if len(dialogsum_df) > SAMPLE_SIZE:\n","        dialogsum_df = dialogsum_df.sample(n=SAMPLE_SIZE, random_state=42).reset_index(drop=True)\n","    else:\n","         dialogsum_df = dialogsum_df.reset_index(drop=True)\n","\n","    print(f\"DialogSum data successfully loaded and sampled to {len(dialogsum_df)} rows.\")\n","    print(\"Proceed to run Colab Cell 2 (Diarization) and Colab Cell 3 (Summarization).\")\n","    print(\"\\nFirst 3 rows of loaded data:\")\n","    print(dialogsum_df.head(3))\n","\n","else:\n","    # This should not happen after the detailed debug, but serves as a final check\n","    print(f\"❌ FATAL ERROR: The required file was not found at {CORRECT_FILE_PATH}. Please check the folder structure again.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V6h6NDPvvNfJ","executionInfo":{"status":"ok","timestamp":1760287119505,"user_tz":-330,"elapsed":167,"user":{"displayName":"Prachiti Morankar","userId":"16577217993084456597"}},"outputId":"451a3a34-75aa-4526-fd1d-59873f03acd2"},"execution_count":60,"outputs":[{"output_type":"stream","name":"stdout","text":["✅ SUCCESS: Found training data at /content/summarization_data/CSV/train.csv. Loading...\n","DialogSum data successfully loaded and sampled to 2000 rows.\n","Proceed to run Colab Cell 2 (Diarization) and Colab Cell 3 (Summarization).\n","\n","First 3 rows of loaded data:\n","                                            dialogue  \\\n","0  #Person1#: I heard you had got a wonderful job...   \n","1  #Person1#: Mr. Lin, what are you interested in...   \n","2  #Person1#: Hi, what will you do with your brok...   \n","\n","                                             summary  \n","0               #Person2# does not like the new job.  \n","1  Mr. Lin tells #Person1# he enjoys camping, hik...  \n","2  #Person2#'ll throw away the broken cell phone ...  \n"]}]},{"cell_type":"code","source":["\n","\n","# STEP 1: DATA PATHS AND FEATURE EXTRACTION (MFCCs)\n","# ORIGINAL: DIARIZATION_DIR = '/content/diarization_data/Mini Speaker Diarization/'\n","# CORRECTED PATH based on debug output:\n","DIARIZATION_DIR = '/content/diarization_data/dataset/'\n","\n","TRAIN_DIR = os.path.join(DIARIZATION_DIR, 'train')\n","SR = 16000 # Sample Rate\n","N_MFCC = 20 # Number of MFCC features\n","LATENT_DIM = 256 # Define LATENT_DIM here for use in Colab Cell 3\n","\n","# --- REST OF THE CODE REMAINS THE SAME ---\n","\n","def extract_mfcc_embedding(audio_path):\n","    \"\"\"Extracts MFCC features and returns a mean-pooled embedding.\"\"\"\n","    # ... (rest of the function code)\n","\n","# STEP 2: CREATE TRAINING DATASET\n","X_train, y_train = [], []\n","# This line will now correctly look inside the 'dataset/train' folder\n","speaker_labels = [name for name in os.listdir(TRAIN_DIR) if os.path.isdir(os.path.join(TRAIN_DIR, name))]\n","label_map = {speaker: i for i, speaker in enumerate(speaker_labels)}\n","# ... (rest of the code for dataset creation and training)\n","# ... (The rest of Colab Cell 2 code follows here)"],"metadata":{"id":"WwgGlbU1x15R","executionInfo":{"status":"ok","timestamp":1760287125116,"user_tz":-330,"elapsed":23,"user":{"displayName":"Prachiti Morankar","userId":"16577217993084456597"}}},"execution_count":61,"outputs":[]},{"cell_type":"code","source":["\n","# --- Training Setup (Requires global constants defined in Cell 1) ---\n","X_train, y_train = [], []\n","\n","# Corrected path variables\n","DIARIZATION_DIR = '/content/diarization_data/dataset/'\n","TRAIN_DIR = os.path.join(DIARIZATION_DIR, 'train')\n","\n","# Create Training Dataset\n","speaker_labels = [name for name in os.listdir(TRAIN_DIR) if os.path.isdir(os.path.join(TRAIN_DIR, name))]\n","label_map = {speaker: i for i, speaker in enumerate(speaker_labels)}\n","\n","print(\"[MODULE 3] Starting MFCC Extraction and Diarization Model Training...\")\n","\n","for speaker, label_idx in label_map.items():\n","    speaker_path = os.path.join(TRAIN_DIR, speaker)\n","    for audio_file in os.listdir(speaker_path):\n","        if audio_file.endswith('.wav'):\n","            embedding = extract_mfcc_embedding(os.path.join(speaker_path, audio_file))\n","            if embedding is not None:\n","                X_train.append(embedding)\n","                y_train.append(label_idx)\n","\n","X_train = np.array(X_train)\n","y_train = np.array(y_train)\n","\n","# --- Train Model ---\n","if X_train.size > 0 and X_train.ndim == 2:\n","    X_tensor = torch.tensor(X_train, dtype=torch.float32)\n","    y_tensor = torch.tensor(y_train, dtype=torch.long)\n","\n","    INPUT_DIM = X_tensor.shape[1]\n","    NUM_SPEAKERS = len(label_map)\n","    diarization_model = SpeakerClassifier(INPUT_DIM, NUM_SPEAKERS)\n","\n","    criterion = torch.nn.CrossEntropyLoss()\n","    optimizer = torch.optim.Adam(diarization_model.parameters(), lr=0.001)\n","\n","    # Simple training loop\n","    for epoch in range(50):\n","        optimizer.zero_grad()\n","        outputs = diarization_model(X_tensor)\n","        loss = criterion(outputs, y_tensor)\n","        loss.backward()\n","        optimizer.step()\n","\n","    # Evaluation Proxy\n","    with torch.no_grad():\n","        outputs = diarization_model(X_tensor)\n","        _, predicted = torch.max(outputs.data, 1)\n","        accuracy = (predicted == y_tensor).sum().item() / len(y_tensor)\n","        print(f\"✅ Diarization Model Training Complete. Accuracy (Proxy for DER): {accuracy*100:.2f}%\")\n","else:\n","     print(\"❌ Diarization Model Training Skipped (No valid training data found).\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F8vVBdukStdT","executionInfo":{"status":"ok","timestamp":1760289095607,"user_tz":-330,"elapsed":630,"user":{"displayName":"Prachiti Morankar","userId":"16577217993084456597"}},"outputId":"ebc8948a-6114-4245-95fb-e114c3c1800a"},"execution_count":83,"outputs":[{"output_type":"stream","name":"stdout","text":["[MODULE 3] Starting MFCC Extraction and Diarization Model Training...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/ffmpeg.py:88: UserWarning: torio.io._streaming_media_decoder.StreamingMediaDecoder has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n","  s = torchaudio.io.StreamReader(src, format, None, buffer_size)\n","/usr/local/lib/python3.12/dist-packages/torchaudio/functional/functional.py:585: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["✅ Diarization Model Training Complete. Accuracy (Proxy for DER): 100.00%\n"]}]},{"cell_type":"code","source":["\n","\n","print(\"[MODULE 4] Starting Seq2Seq Summarizer Model Training...\")\n","\n","# Data Preprocessing/Tokenization (Data variables defined in Cell 1)\n","input_texts = dialogsum_df['dialogue'].astype(str).tolist()\n","target_texts = dialogsum_df['summary'].astype(str).apply(lambda x: 'sostok ' + x + ' eostok').tolist()\n","\n","# Re-create the padded input data (required for training and model dimensions)\n","input_sequences = input_tokenizer.texts_to_sequences(input_texts)\n","encoder_input_data = pad_sequences(input_sequences, maxlen=MAX_LEN_DIALOGUE, padding='post')\n","\n","target_sequences = target_tokenizer.texts_to_sequences(target_texts)\n","decoder_input_data = pad_sequences(target_sequences, maxlen=MAX_LEN_SUMMARY, padding='post')\n","\n","decoder_target_data = np.zeros(\n","    (len(decoder_input_data), MAX_LEN_SUMMARY, TARGET_VOCAB_SIZE),\n","    dtype='float32'\n",")\n","for i, seq in enumerate(decoder_input_data):\n","    for t, word_index in enumerate(seq):\n","        if t > 0 and word_index != 0:\n","            decoder_target_data[i, t-1, word_index] = 1.0\n","\n","\n","# Seq2Seq Model Architecture (Re-define to build)\n","encoder_inputs = Input(shape=(MAX_LEN_DIALOGUE,))\n","encoder_emb = Embedding(INPUT_VOCAB_SIZE, 128, mask_zero=True)(encoder_inputs)\n","encoder_lstm, state_h, state_c = LSTM(LATENT_DIM, return_state=True)(encoder_emb)\n","encoder_states = [state_h, state_c]\n","\n","decoder_inputs = Input(shape=(None,))\n","decoder_emb = Embedding(TARGET_VOCAB_SIZE, 128, mask_zero=True)(decoder_inputs)\n","decoder_lstm = LSTM(LATENT_DIM, return_sequences=True, return_state=True, name='decoder_lstm')\n","decoder_outputs, _, _ = decoder_lstm(decoder_emb, initial_state=encoder_states)\n","decoder_dense = Dense(TARGET_VOCAB_SIZE, activation='softmax')\n","decoder_outputs = decoder_dense(decoder_outputs)\n","\n","# Final Training Model\n","summarization_training_model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n","summarization_training_model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n","\n","# Train Model\n","print(\"Starting summarization_training_model.fit()...\")\n","summarization_training_model.fit(\n","    [encoder_input_data, decoder_input_data],\n","    decoder_target_data,\n","    batch_size=64,\n","    epochs=10,\n","    validation_split=0.1,\n","    verbose=1\n",")\n","print(\"✅ Seq2Seq Summarizer Model Training Complete.\")\n","\n","# ----------------------------------------------------------------------\n","# Build Inference Models (CRITICAL STEP for Integrated Pipeline)\n","# ----------------------------------------------------------------------\n","encoder_model = Model(encoder_inputs, encoder_states)\n","\n","decoder_state_input_h = Input(shape=(LATENT_DIM,))\n","decoder_state_input_c = Input(shape=(LATENT_DIM,))\n","decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n","\n","decoder_outputs2, state_h2, state_c2 = decoder_lstm(\n","    decoder_emb, initial_state=decoder_states_inputs\n",")\n","decoder_states2 = [state_h2, state_c2]\n","decoder_outputs2 = decoder_dense(decoder_outputs2)\n","\n","decoder_model = Model(\n","    [decoder_inputs] + decoder_states_inputs,\n","    [decoder_outputs2] + decoder_states2\n",")\n","\n","# Inference Function (required by Colab Cell 4)\n","def decode_sequence(input_seq):\n","    states_value = encoder_model.predict(input_seq, verbose=0)\n","    target_seq = np.zeros((1, 1))\n","    target_seq[0, 0] = target_tokenizer.word_index['sostok']\n","    decoded_sentence = ''\n","\n","    stop_condition = False\n","    reverse_target_word_index = {v: k for k, v in target_tokenizer.word_index.items()}\n","\n","    while not stop_condition:\n","        output_tokens, h, c = decoder_model.predict([target_seq] + states_value, verbose=0)\n","        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n","        sampled_word = reverse_target_word_index.get(sampled_token_index, '')\n","\n","        if sampled_word not in ('eostok', ''):\n","            decoded_sentence += ' ' + sampled_word\n","\n","        if (sampled_word == 'eostok' or len(decoded_sentence.split()) > MAX_LEN_SUMMARY):\n","            stop_condition = True\n","\n","        target_seq = np.zeros((1, 1))\n","        target_seq[0, 0] = sampled_token_index\n","        states_value = [h, c]\n","\n","    return decoded_sentence.strip()\n","\n","print(\"✅ Milestone 2 Summarizer components are ready. Proceed to run Colab Cell 4.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9S8tzvgNTFge","executionInfo":{"status":"ok","timestamp":1760288080996,"user_tz":-330,"elapsed":794921,"user":{"displayName":"Prachiti Morankar","userId":"16577217993084456597"}},"outputId":"cdb6c622-8a18-4b8f-c493-e964ca483cbe"},"execution_count":65,"outputs":[{"output_type":"stream","name":"stdout","text":["[MODULE 4] Starting Seq2Seq Summarizer Model Training...\n","Starting summarization_training_model.fit()...\n","Epoch 1/10\n","\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 3s/step - loss: 7.9013 - val_loss: 5.7854\n","Epoch 2/10\n","\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 3s/step - loss: 5.9399 - val_loss: 5.6746\n","Epoch 3/10\n","\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 3s/step - loss: 5.8291 - val_loss: 5.6459\n","Epoch 4/10\n","\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 3s/step - loss: 5.8161 - val_loss: 5.6165\n","Epoch 5/10\n","\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 3s/step - loss: 5.7845 - val_loss: 5.5976\n","Epoch 6/10\n","\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 3s/step - loss: 5.7867 - val_loss: 5.6005\n","Epoch 7/10\n","\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 3s/step - loss: 5.7526 - val_loss: 5.5729\n","Epoch 8/10\n","\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 3s/step - loss: 5.7525 - val_loss: 5.5690\n","Epoch 9/10\n","\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 3s/step - loss: 5.7354 - val_loss: 5.5609\n","Epoch 10/10\n","\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 3s/step - loss: 5.6864 - val_loss: 5.5253\n","✅ Seq2Seq Summarizer Model Training Complete.\n","✅ Milestone 2 Summarizer components are ready. Proceed to run Colab Cell 4.\n"]}]},{"cell_type":"code","source":["\n","# Dependencies are assumed to be installed and models defined from preceding cells.\n","import os\n","import time\n","import base64\n","from IPython.display import Javascript, display\n","from google.colab.output import eval_js # Keep this import for eval_js\n","\n","AUDIO_FILE_NAME = 'live_meeting_audio.wav'\n","\n","# --- CORRECTED record_audio FUNCTION ---\n","def record_audio(filename='audio.wav', duration_sec=15):\n","    \"\"\"Captures audio from the microphone using a stable Colab JS/Python bridge.\"\"\"\n","\n","    # 1. HTML/JS to set up status and buttons\n","    js_setup = \"\"\"\n","        const statusDiv = document.createElement('div');\n","        statusDiv.id = 'recording-status';\n","        statusDiv.innerHTML = 'Status: Ready...';\n","        document.body.appendChild(statusDiv);\n","    \"\"\"\n","    display(Javascript(js_setup))\n","    print(f\"Recording for {duration_sec} seconds... PRESS STOP AT ANY TIME.\")\n","\n","    # 2. JS function for recording and sending data - ***FIXED SYNTAX HERE***\n","    js_code = f\"\"\"\n","        (async function() {{ // IMMEDIATELY INVOKED ASYNC FUNCTION WRAPPER\n","            document.getElementById('recording-status').innerHTML = 'Status: Recording...';\n","\n","            // --- Core Audio Capture Logic ---\n","            // FIX: 'await' is now valid inside this async function\n","            const audio = await navigator.mediaDevices.getUserMedia({{audio: true}});\n","            const mediaRecorder = new MediaRecorder(audio);\n","            const audioChunks = [];\n","\n","            mediaRecorder.ondataavailable = e => {{ audioChunks.push(e.data); }};\n","\n","            // Promise to handle the stop event and return base64 data\n","            const recordingPromise = new Promise(resolve => {{\n","                mediaRecorder.onstop = () => {{\n","                    document.getElementById('recording-status').innerHTML = 'Status: Processing...';\n","                    const audioBlob = new Blob(audioChunks, {{'type': 'audio/ogg; codecs=opus'}});\n","                    const fileReader = new FileReader();\n","                    fileReader.readAsDataURL(audioBlob);\n","                    fileReader.onloadend = () => {{\n","                        const base64data = fileReader.result.split(',')[1];\n","                        resolve(base64data);\n","                    }}\n","                }};\n","            }});\n","\n","            mediaRecorder.start();\n","\n","            let timer = setTimeout(() => {{\n","                if (mediaRecorder.state !== 'inactive') {{\n","                    mediaRecorder.stop();\n","                    document.getElementById('recording-status').innerHTML = 'Status: Timeout (Stopped).';\n","                }}\n","            }}, {duration_sec * 1000});\n","\n","            // UI button to stop early\n","            const button = document.createElement('button');\n","            button.innerHTML = 'STOP RECORDING';\n","            button.onclick = () => {{\n","                if (mediaRecorder.state !== 'inactive') {{\n","                    mediaRecorder.stop();\n","                    clearTimeout(timer);\n","                    document.getElementById('recording-status').innerHTML = 'Status: User Stopped.';\n","                }}\n","            }};\n","            document.body.appendChild(button);\n","\n","            // Return the base64 data to Python\n","            return recordingPromise;\n","        }})() // INVOKE THE ASYNC FUNCTION\n","    \"\"\"\n","\n","    # Execute the JS code and wait for the base64 data\n","    base64_data = eval_js(js_code)\n","\n","    # Save the file using Python\n","    audio_data = base64.b64decode(base64_data)\n","    with open(filename, 'wb') as f:\n","      f.write(audio_data)\n","\n","    print(f\"\\n✅ Audio saved to {filename}\")\n","\n","\n","# --- RERUN EXECUTION BLOCK ---\n","print(\"--- Step 1: Capturing Live Audio (M1) ---\")\n","try:\n","    record_audio(AUDIO_FILE_NAME, duration_sec=15)\n","    # The rest of the Colab Cell 4 logic follows here: STT, Diarization, Summarization\n","except Exception as e:\n","    print(f\"❌ Critical Error during audio capture (Python): {e}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":125},"id":"cuqQ1YedTTHV","executionInfo":{"status":"ok","timestamp":1760291020231,"user_tz":-330,"elapsed":15904,"user":{"displayName":"Prachiti Morankar","userId":"16577217993084456597"}},"outputId":"a32009ab-3247-42f7-f7e3-ca672d3a06c1"},"execution_count":87,"outputs":[{"output_type":"stream","name":"stdout","text":["--- Step 1: Capturing Live Audio (M1) ---\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","        const statusDiv = document.createElement('div');\n","        statusDiv.id = 'recording-status';\n","        statusDiv.innerHTML = 'Status: Ready...';\n","        document.body.appendChild(statusDiv);\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Recording for 15 seconds... PRESS STOP AT ANY TIME.\n","\n","✅ Audio saved to live_meeting_audio.wav\n"]}]},{"cell_type":"code","source":["\n","import os\n","import torch\n","import re\n","from transformers import pipeline\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","# --- Ensure AUDIO_FILE_NAME is defined from the previous cell ---\n","AUDIO_FILE_NAME = 'live_meeting_audio.wav'\n","\n","# --- FFMPEG Conversion (Prepares the file for Whisper STT) ---\n","print(\"\\n--- Running FFMPEG Conversion ---\")\n","# FFMPEG is required to convert the browser's OGG audio to a 16kHz WAV format\n","!ffmpeg -i {AUDIO_FILE_NAME} -ar 16000 -ac 1 temp_16k_mono.wav -y\n","print(\"✅ Conversion to temp_16k_mono.wav complete.\")\n","\n","# --- STT (M1 Component) ---\n","print(\"\\n--- Step 2.1: Transcription (STT) ---\")\n","# Load the Whisper pipeline\n","whisper_pipeline = pipeline(\n","    \"automatic-speech-recognition\",\n","    model=\"openai/whisper-tiny.en\",\n","    device=0 if torch.cuda.is_available() else -1\n",")\n","raw_transcript = whisper_pipeline(\"temp_16k_mono.wav\")['text'].strip()\n","print(f\"Raw Transcript: {raw_transcript}\")\n","\n","# --- Diarization/Alignment (M2 Component) ---\n","# NOTE: Using MOCK Diarization for stability\n","def mock_diarization(transcript):\n","    turns = re.split(r'[.?!]', transcript)\n","    output = \"\"\n","    speakers = [\"Speaker A\", \"Speaker B\", \"Speaker C\"]\n","    for i, turn in enumerate(turns):\n","        if turn.strip():\n","            # Add period back after the turn\n","            output += f\"[{speakers[i % len(speakers)]}]: {turn.strip()}.\"\n","    return output\n","\n","diarized_transcript = mock_diarization(raw_transcript)\n","print(\"\\n--- Diarized Transcript (Mock Alignment) ---\")\n","print(diarized_transcript)\n","\n","# --- Summarization (M2 Component) ---\n","print(\"\\n--- Step 3: Summarization ---\")\n","try:\n","    # Tokenize the new diarized input (Requires global tokenizers from Cell 1/3)\n","    new_input_sequence = input_tokenizer.texts_to_sequences([diarized_transcript])\n","    new_encoder_input = pad_sequences(new_input_sequence, maxlen=MAX_LEN_DIALOGUE, padding='post')\n","\n","    # Generate the summary (Requires decode_sequence function from Cell 3)\n","    final_summary = decode_sequence(new_encoder_input)\n","\n","    print(\"✅ Final Pipeline Complete.\")\n","    print(\"---------------------------------------\")\n","    print(f\"FINAL SUMMARY: {final_summary}\")\n","    print(\"---------------------------------------\")\n","\n","except NameError as e:\n","    print(f\"❌ ERROR: Inference models/variables not found: {e}\")\n","    print(\"Ensure all setup cells (1-3) were run COMPLETELY to define input_tokenizer, decode_sequence, etc.\")\n","except Exception as e:\n","    print(f\"❌ An unexpected error occurred during summarization: {e}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zyyOqpfmYdFc","executionInfo":{"status":"ok","timestamp":1760291031898,"user_tz":-330,"elapsed":5822,"user":{"displayName":"Prachiti Morankar","userId":"16577217993084456597"}},"outputId":"20ed9688-bd5e-467e-97a1-bfbfd4c25b53"},"execution_count":88,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Running FFMPEG Conversion ---\n","ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n","  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n","  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n","  libavutil      56. 70.100 / 56. 70.100\n","  libavcodec     58.134.100 / 58.134.100\n","  libavformat    58. 76.100 / 58. 76.100\n","  libavdevice    58. 13.100 / 58. 13.100\n","  libavfilter     7.110.100 /  7.110.100\n","  libswscale      5.  9.100 /  5.  9.100\n","  libswresample   3.  9.100 /  3.  9.100\n","  libpostproc    55.  9.100 / 55.  9.100\n","Input #0, matroska,webm, from 'live_meeting_audio.wav':\n","  Metadata:\n","    encoder         : Chrome\n","  Duration: 00:00:14.88, start: 0.000000, bitrate: 129 kb/s\n","  Stream #0:0(eng): Audio: opus, 48000 Hz, mono, fltp (default)\n","Stream mapping:\n","  Stream #0:0 -> #0:0 (opus (native) -> pcm_s16le (native))\n","Press [q] to stop, [?] for help\n","Output #0, wav, to 'temp_16k_mono.wav':\n","  Metadata:\n","    ISFT            : Lavf58.76.100\n","  Stream #0:0(eng): Audio: pcm_s16le ([1][0][0][0] / 0x0001), 16000 Hz, mono, s16, 256 kb/s (default)\n","    Metadata:\n","      encoder         : Lavc58.134.100 pcm_s16le\n","size=     467kB time=00:00:14.93 bitrate= 256.1kbits/s speed= 232x    \n","video:0kB audio:467kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.016315%\n","✅ Conversion to temp_16k_mono.wav complete.\n","\n","--- Step 2.1: Transcription (STT) ---\n"]},{"output_type":"stream","name":"stderr","text":["Device set to use cpu\n"]},{"output_type":"stream","name":"stdout","text":["Raw Transcript: The stale smell of cold beer fingers. It takes heat to bring out the odor. In cold dip restores health.\n","\n","--- Diarized Transcript (Mock Alignment) ---\n","[Speaker A]: The stale smell of cold beer fingers.[Speaker B]: It takes heat to bring out the odor.[Speaker C]: In cold dip restores health.\n","\n","--- Step 3: Summarization ---\n","✅ Final Pipeline Complete.\n","---------------------------------------\n","FINAL SUMMARY: person1 the\n","---------------------------------------\n"]}]}]}